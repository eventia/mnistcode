{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aicode12.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPx9AzB1nrFTkybfL1BxxmD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eventia/mnistcode/blob/master/aicode12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLhDZ5t5l2hs",
        "colab_type": "text"
      },
      "source": [
        "# 고등학교 수학과 파이썬으로 배우는 인공지능 소스코드  **Ch 12**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqAGx5R7lxng",
        "colab_type": "code",
        "outputId": "97c660be-d3ec-470a-8f57-840ed3cf20ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# 12.1. MNIST 데이터 입력\n",
        "\n",
        "import numpy as np \n",
        "from keras.datasets import mnist\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "t_trainlbl, t_testlbl = t_train, t_test\n",
        "\n",
        "# 28X28 을 784 로 수정\n",
        "x_train = x_train.reshape(60000,784)    # 주석 (1)\n",
        "x_test = x_test.reshape(10000,784)    \n",
        "\n",
        "# one-hot label \n",
        "T0 = np.zeros((t_train.size, 10))    #(60000,10) = 000\n",
        "T1 = np.zeros((t_test.size, 10))    #(10000,10) = 000\n",
        "\n",
        "for idx in range(t_train.size): T0[idx][t_train[idx]] = 1    #(3))\n",
        "for idx in range(t_test.size): T1[idx][t_test[idx]] = 1\n",
        "\n",
        "t_train, t_test = T0, T1\n",
        "\n",
        "# normalize 0.0 ~ 1.0\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "print('MNIST DataSets 준비 완료')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "MNIST DataSets 준비 완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K33feUtOma2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 12.2. 함수정의\n",
        "\n",
        "# 미분함수 \n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4    # 0.0001\n",
        "    nd_coef = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        index = it.multi_index\n",
        "        tmp = x[index]\n",
        "        x[index] = tmp + h\n",
        "        fxh2 = f()    # f(x+h)\n",
        "        x[index] = tmp - h \n",
        "        fxh1 = f()    # f(x-h)\n",
        "        nd_coef[index] = (fxh2 - fxh1) / (2*h)\n",
        "        x[index] = tmp \n",
        "        it.iternext()\n",
        "    return nd_coef\n",
        "\n",
        "# 시그모이드\n",
        "def sigmoid(x):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "\n",
        "# 소프트맥스\n",
        "def softmax(x):\n",
        "    if x.ndim == 1:  # 기본 1개 처리과정 , 벡터입력\n",
        "        x = x - np.max(x) \n",
        "        return np.exp(x) / np.sum(np.exp(x))\n",
        "    if x.ndim == 2:  # 배치용 n 개 처리, 행렬입력\n",
        "        x = x.T - np.max(x.T, axis=0)\n",
        "        return (np.exp(x) / np.sum(np.exp(x), axis=0)).T\n",
        "\n",
        "# 크로스엔트로피오차\n",
        "def cee(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)  # 크기가 1xN 인 2차원 행렬로 재구성\n",
        "        y = y.reshape(1, y.size)\n",
        "    result = -np.sum(t * np.log(y + 1e-7))  / y.shape[0] \n",
        "    return result \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "666-Y5NQm0Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 12.3. 프로세스별 클래스 생성\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        result = x.copy()\n",
        "        result[self.mask] = 0\n",
        "        return result\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = sigmoid(x)\n",
        "        return self.out\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W    # W0, W1\n",
        "        self.b = b    # b0, b1\n",
        "        self.x = None\n",
        "        self.dW = None    # W0, W1 의 기울기\n",
        "        self.db = None    # b0, b1 의 기울기\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        result = np.dot(self.x, self.W) + self.b\n",
        "        return result\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.y = None    # 출력(계산결과)\n",
        "        self.t = None    # 정답(MNIST레이블)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        result = cee(self.y, self.t)\n",
        "        return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x74EbT0anN9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 12.4. 네트워크클래스 생성\n",
        "\n",
        "class SimpleNetwork:\n",
        "    def __init__(self, inputx, hidden, outy, weight):\n",
        "        # 가중치 초기화\n",
        "        self.netMat = {}\n",
        "        self.netMat['W0'] = weight * np.random.randn(inputx, hidden)\n",
        "        self.netMat['b0'] = np.zeros(hidden)\n",
        "        self.netMat['W1'] = weight * np.random.randn(hidden, outy) \n",
        "        self.netMat['b1'] = np.zeros(outy)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.netLayers = {}\n",
        "        self.netLayers['Affine1'] = Affine(self.netMat['W0'], \n",
        "                                           self.netMat['b0'])\n",
        "        self.netLayers['Relu1'] = Relu()\n",
        "        self.netLayers['Affine2'] = Affine(self.netMat['W1'], \n",
        "                                           self.netMat['b1'])\n",
        "        self.netLayers['Softmax'] = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = self.netLayers['Affine1'].forward(x)\n",
        "        x = self.netLayers['Relu1'].forward(x)\n",
        "        x = self.netLayers['Affine2'].forward(x)\n",
        "        return x\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.netLayers['Softmax'].forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    def numerical_gradient(self, x, t):\n",
        "        lossfunc = lambda : self.loss(x, t)\n",
        "        grads = {}\n",
        "        grads['W0'] = numerical_diff(lossfunc, self.netMat['W0'])\n",
        "        grads['b0'] = numerical_diff(lossfunc, self.netMat['b0'])\n",
        "        grads['W1'] = numerical_diff(lossfunc, self.netMat['W1'])\n",
        "        grads['b1'] = numerical_diff(lossfunc, self.netMat['b1'])\n",
        "        return grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaL2drODn9PH",
        "colab_type": "code",
        "outputId": "c18c7fa4-c0f3-4882-c945-08ed8f939bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 12.5. 미분을 이용한 학습과 검증\n",
        "\n",
        "import time    # Using Time Module(시간측정)\n",
        "t1 = time.time()    # save nowTime(현재 시간 측정)\n",
        "\n",
        "train_size = x_train.shape[0]    # size of TrainData (입력데이터 크기) 60000\n",
        "lr = 0.1    # learning rate(학습률)\n",
        "iter = 0    # Iternation Number (반복횟수)\n",
        "\n",
        "iters_num = 1000\n",
        "batch_size = 20\n",
        "iter_per_epoch = 1\n",
        "\n",
        "network = SimpleNetwork(inputx=784, hidden=50, outy=10, weight = 0.2)\n",
        "\n",
        "print('loss = _______  time = ________  n = ______ | [TrainAcc] [TestAcc]')\n",
        "\n",
        "for i in range(iters_num):    # 1000\n",
        "    batch_mask = np.random.choice(train_size, batch_size)    #(1)주석 \n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분(편 미분) 방식\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W0', 'b0', 'W1', 'b1'): network.netMat[key] -= lr * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    iter = iter + 1\n",
        "\n",
        "    print('loss = {:7.4f}  '.format(loss), end='')\n",
        "    print('time = {:8.4f}  '.format(time.time()-t1), end='')    \n",
        "    print('n = {:06d} |  {:6.4f}  {:9.4f}'.format(iter, train_acc, test_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss = _______  time = ________  n = ______ | [TrainAcc] [TestAcc]\n",
            "loss =  2.1324  time =  15.1599  n = 000001 |  0.1068     0.1097\n",
            "loss =  2.5134  time =  30.0661  n = 000002 |  0.1537     0.1625\n",
            "loss =  1.8259  time =  44.9280  n = 000003 |  0.1613     0.1622\n",
            "loss =  1.3325  time =  59.7791  n = 000004 |  0.1893     0.1909\n",
            "loss =  1.8377  time =  74.5542  n = 000005 |  0.2281     0.2304\n",
            "loss =  1.8078  time =  89.5424  n = 000006 |  0.2567     0.2607\n",
            "loss =  1.5525  time = 104.8017  n = 000007 |  0.2727     0.2761\n",
            "loss =  1.6940  time = 119.8533  n = 000008 |  0.3178     0.3295\n",
            "loss =  1.2581  time = 134.6920  n = 000009 |  0.3338     0.3477\n",
            "loss =  1.3563  time = 149.8605  n = 000010 |  0.3388     0.3494\n",
            "loss =  1.3285  time = 164.9146  n = 000011 |  0.3514     0.3616\n",
            "loss =  0.9924  time = 179.8845  n = 000012 |  0.4269     0.4331\n",
            "loss =  1.1199  time = 194.6569  n = 000013 |  0.4235     0.4265\n",
            "loss =  1.0018  time = 209.3665  n = 000014 |  0.4527     0.4634\n",
            "loss =  1.2897  time = 224.2416  n = 000015 |  0.4431     0.4472\n",
            "loss =  1.2232  time = 238.9122  n = 000016 |  0.4567     0.4642\n",
            "loss =  1.1677  time = 253.5160  n = 000017 |  0.4839     0.4963\n",
            "loss =  1.2078  time = 268.4257  n = 000018 |  0.4921     0.5083\n",
            "loss =  1.1158  time = 283.3915  n = 000019 |  0.4947     0.5053\n",
            "loss =  0.8043  time = 298.1204  n = 000020 |  0.4974     0.5090\n",
            "loss =  1.3308  time = 312.8575  n = 000021 |  0.4933     0.5069\n",
            "loss =  1.3522  time = 327.7350  n = 000022 |  0.5624     0.5696\n",
            "loss =  0.8172  time = 342.4774  n = 000023 |  0.5480     0.5577\n",
            "loss =  0.8713  time = 357.4035  n = 000024 |  0.5599     0.5784\n",
            "loss =  0.8427  time = 372.3505  n = 000025 |  0.5662     0.5869\n",
            "loss =  1.1615  time = 386.9675  n = 000026 |  0.5898     0.6058\n",
            "loss =  0.8508  time = 401.7606  n = 000027 |  0.6016     0.6141\n",
            "loss =  0.8363  time = 416.6858  n = 000028 |  0.6023     0.6162\n",
            "loss =  0.5619  time = 431.4138  n = 000029 |  0.5973     0.6059\n",
            "loss =  0.6137  time = 446.0577  n = 000030 |  0.6082     0.6235\n",
            "loss =  0.7206  time = 461.0648  n = 000031 |  0.6096     0.6242\n",
            "loss =  0.6346  time = 475.7196  n = 000032 |  0.5974     0.6137\n",
            "loss =  0.7019  time = 490.3629  n = 000033 |  0.6010     0.6131\n",
            "loss =  0.5075  time = 505.1288  n = 000034 |  0.6006     0.6190\n",
            "loss =  0.7389  time = 520.1258  n = 000035 |  0.6170     0.6312\n",
            "loss =  0.7566  time = 535.1346  n = 000036 |  0.6165     0.6331\n",
            "loss =  0.7552  time = 550.0993  n = 000037 |  0.6643     0.6797\n",
            "loss =  0.6245  time = 564.7724  n = 000038 |  0.6554     0.6652\n",
            "loss =  0.7132  time = 579.6294  n = 000039 |  0.6489     0.6566\n",
            "loss =  0.6023  time = 594.3365  n = 000040 |  0.6763     0.6815\n",
            "loss =  0.5964  time = 609.2421  n = 000041 |  0.6736     0.6791\n",
            "loss =  0.9855  time = 624.0944  n = 000042 |  0.6730     0.6856\n",
            "loss =  0.5808  time = 639.2775  n = 000043 |  0.6567     0.6652\n",
            "loss =  0.3754  time = 654.0972  n = 000044 |  0.6936     0.7055\n",
            "loss =  0.6089  time = 668.7711  n = 000045 |  0.7112     0.7228\n",
            "loss =  0.5219  time = 683.3470  n = 000046 |  0.6940     0.7110\n",
            "loss =  0.8494  time = 698.1481  n = 000047 |  0.6855     0.7046\n",
            "loss =  0.7092  time = 712.9376  n = 000048 |  0.6984     0.7135\n",
            "loss =  0.7152  time = 727.6622  n = 000049 |  0.6930     0.7028\n",
            "loss =  0.4452  time = 742.3206  n = 000050 |  0.7098     0.7193\n",
            "loss =  0.5139  time = 757.3706  n = 000051 |  0.7016     0.7161\n",
            "loss =  0.6387  time = 772.3564  n = 000052 |  0.7267     0.7439\n",
            "loss =  0.4739  time = 786.8863  n = 000053 |  0.6738     0.6849\n",
            "loss =  0.4703  time = 801.5837  n = 000054 |  0.6968     0.7142\n",
            "loss =  0.5084  time = 816.3750  n = 000055 |  0.7305     0.7426\n",
            "loss =  0.5343  time = 831.1171  n = 000056 |  0.7363     0.7493\n",
            "loss =  0.6101  time = 845.9504  n = 000057 |  0.6543     0.6602\n",
            "loss =  0.6662  time = 860.6400  n = 000058 |  0.7344     0.7463\n",
            "loss =  0.7760  time = 875.6038  n = 000059 |  0.7437     0.7528\n",
            "loss =  0.4139  time = 890.5511  n = 000060 |  0.7410     0.7507\n",
            "loss =  0.3570  time = 905.4800  n = 000061 |  0.7362     0.7405\n",
            "loss =  0.3122  time = 919.8972  n = 000062 |  0.7032     0.7156\n",
            "loss =  0.4796  time = 934.7344  n = 000063 |  0.7194     0.7299\n",
            "loss =  0.2810  time = 949.4681  n = 000064 |  0.7321     0.7426\n",
            "loss =  0.6529  time = 964.1094  n = 000065 |  0.7529     0.7642\n",
            "loss =  0.5251  time = 978.7473  n = 000066 |  0.7463     0.7601\n",
            "loss =  0.7941  time = 993.8848  n = 000067 |  0.7534     0.7670\n",
            "loss =  0.5785  time = 1008.4921  n = 000068 |  0.7204     0.7368\n",
            "loss =  0.2563  time = 1022.9855  n = 000069 |  0.7105     0.7173\n",
            "loss =  0.5684  time = 1037.6269  n = 000070 |  0.7584     0.7691\n",
            "loss =  0.4826  time = 1052.5826  n = 000071 |  0.7657     0.7697\n",
            "loss =  0.4274  time = 1067.1035  n = 000072 |  0.7559     0.7637\n",
            "loss =  0.4633  time = 1081.9530  n = 000073 |  0.7530     0.7592\n",
            "loss =  0.7237  time = 1096.5494  n = 000074 |  0.7093     0.7176\n",
            "loss =  0.4830  time = 1111.3211  n = 000075 |  0.7714     0.7810\n",
            "loss =  0.3279  time = 1126.1476  n = 000076 |  0.7626     0.7725\n",
            "loss =  0.3908  time = 1140.8187  n = 000077 |  0.7502     0.7591\n",
            "loss =  0.3706  time = 1155.4467  n = 000078 |  0.7700     0.7790\n",
            "loss =  0.3703  time = 1170.4036  n = 000079 |  0.7540     0.7607\n",
            "loss =  0.6053  time = 1185.1557  n = 000080 |  0.7512     0.7520\n",
            "loss =  0.7610  time = 1199.9505  n = 000081 |  0.7678     0.7708\n",
            "loss =  0.2954  time = 1214.5609  n = 000082 |  0.7558     0.7593\n",
            "loss =  0.4193  time = 1229.2892  n = 000083 |  0.6966     0.7075\n",
            "loss =  0.4638  time = 1244.1214  n = 000084 |  0.7313     0.7369\n",
            "loss =  0.8369  time = 1258.8113  n = 000085 |  0.7531     0.7628\n",
            "loss =  0.4717  time = 1273.4998  n = 000086 |  0.7781     0.7849\n",
            "loss =  0.3566  time = 1288.2399  n = 000087 |  0.7657     0.7693\n",
            "loss =  0.5154  time = 1303.0586  n = 000088 |  0.7817     0.7899\n",
            "loss =  0.6179  time = 1317.6424  n = 000089 |  0.7646     0.7722\n",
            "loss =  0.3296  time = 1332.2941  n = 000090 |  0.7281     0.7366\n",
            "loss =  0.3806  time = 1347.3266  n = 000091 |  0.7243     0.7300\n",
            "loss =  0.6604  time = 1362.2396  n = 000092 |  0.7577     0.7623\n",
            "loss =  0.3581  time = 1377.0804  n = 000093 |  0.7745     0.7779\n",
            "loss =  0.6362  time = 1391.5261  n = 000094 |  0.7809     0.7854\n",
            "loss =  0.2486  time = 1405.9904  n = 000095 |  0.7658     0.7723\n",
            "loss =  0.5171  time = 1420.9811  n = 000096 |  0.7707     0.7762\n",
            "loss =  0.4296  time = 1435.6732  n = 000097 |  0.7925     0.7967\n",
            "loss =  0.2559  time = 1450.1899  n = 000098 |  0.7707     0.7800\n",
            "loss =  0.4828  time = 1464.7828  n = 000099 |  0.7899     0.7972\n",
            "loss =  0.3687  time = 1479.7092  n = 000100 |  0.7979     0.8051\n",
            "loss =  0.2921  time = 1494.4241  n = 000101 |  0.7813     0.7862\n",
            "loss =  0.2859  time = 1509.1116  n = 000102 |  0.7724     0.7750\n",
            "loss =  0.5259  time = 1523.5553  n = 000103 |  0.7463     0.7483\n",
            "loss =  0.3438  time = 1538.3213  n = 000104 |  0.7535     0.7542\n",
            "loss =  0.5696  time = 1553.0972  n = 000105 |  0.7766     0.7838\n",
            "loss =  0.5710  time = 1567.5446  n = 000106 |  0.7341     0.7366\n",
            "loss =  0.4883  time = 1582.0893  n = 000107 |  0.7605     0.7642\n",
            "loss =  0.3423  time = 1596.8838  n = 000108 |  0.7799     0.7829\n",
            "loss =  0.4386  time = 1611.7918  n = 000109 |  0.7875     0.7901\n",
            "loss =  0.3526  time = 1626.5741  n = 000110 |  0.7804     0.7854\n",
            "loss =  0.2373  time = 1641.1747  n = 000111 |  0.7918     0.7979\n",
            "loss =  0.4498  time = 1656.3129  n = 000112 |  0.7903     0.7973\n",
            "loss =  0.3256  time = 1671.3598  n = 000113 |  0.7954     0.8035\n",
            "loss =  0.6128  time = 1685.9918  n = 000114 |  0.8052     0.8109\n",
            "loss =  0.4530  time = 1700.6136  n = 000115 |  0.7865     0.7916\n",
            "loss =  0.4971  time = 1715.4463  n = 000116 |  0.8050     0.8066\n",
            "loss =  0.1658  time = 1730.2439  n = 000117 |  0.7956     0.8030\n",
            "loss =  0.3232  time = 1744.8201  n = 000118 |  0.7962     0.7983\n",
            "loss =  0.5188  time = 1759.2684  n = 000119 |  0.8065     0.8126\n",
            "loss =  0.3030  time = 1774.3927  n = 000120 |  0.8029     0.8069\n",
            "loss =  0.3416  time = 1789.1206  n = 000121 |  0.7932     0.7939\n",
            "loss =  0.5098  time = 1803.7157  n = 000122 |  0.8096     0.8140\n",
            "loss =  0.6256  time = 1818.3636  n = 000123 |  0.8042     0.8087\n",
            "loss =  0.4259  time = 1833.3669  n = 000124 |  0.8152     0.8163\n",
            "loss =  0.3237  time = 1848.0607  n = 000125 |  0.8046     0.8081\n",
            "loss =  0.6491  time = 1862.9483  n = 000126 |  0.8137     0.8196\n",
            "loss =  0.2236  time = 1877.5923  n = 000127 |  0.7910     0.7954\n",
            "loss =  0.2539  time = 1892.4613  n = 000128 |  0.7967     0.7967\n",
            "loss =  0.2059  time = 1907.1947  n = 000129 |  0.8030     0.8039\n",
            "loss =  0.4529  time = 1921.9353  n = 000130 |  0.8115     0.8108\n",
            "loss =  0.2530  time = 1936.7638  n = 000131 |  0.8183     0.8244\n",
            "loss =  0.2947  time = 1951.5157  n = 000132 |  0.8189     0.8226\n",
            "loss =  0.1347  time = 1966.3540  n = 000133 |  0.8120     0.8144\n",
            "loss =  0.5311  time = 1981.4578  n = 000134 |  0.8200     0.8248\n",
            "loss =  0.3273  time = 1996.0817  n = 000135 |  0.7877     0.7930\n",
            "loss =  0.2725  time = 2010.8520  n = 000136 |  0.8128     0.8183\n",
            "loss =  0.2630  time = 2025.5158  n = 000137 |  0.8186     0.8211\n",
            "loss =  0.3459  time = 2040.4109  n = 000138 |  0.8136     0.8179\n",
            "loss =  0.4875  time = 2054.9072  n = 000139 |  0.7931     0.7986\n",
            "loss =  0.2695  time = 2069.7487  n = 000140 |  0.7813     0.7850\n",
            "loss =  0.8601  time = 2084.6766  n = 000141 |  0.7939     0.8020\n",
            "loss =  0.4895  time = 2099.4630  n = 000142 |  0.7726     0.7778\n",
            "loss =  0.5096  time = 2113.9647  n = 000143 |  0.7928     0.8022\n",
            "loss =  0.2722  time = 2128.4809  n = 000144 |  0.7902     0.7932\n",
            "loss =  0.5099  time = 2143.2260  n = 000145 |  0.8205     0.8283\n",
            "loss =  0.3912  time = 2157.7765  n = 000146 |  0.8019     0.8098\n",
            "loss =  0.5043  time = 2172.3205  n = 000147 |  0.8001     0.8062\n",
            "loss =  0.6252  time = 2186.9729  n = 000148 |  0.8278     0.8323\n",
            "loss =  0.2695  time = 2201.8847  n = 000149 |  0.8186     0.8271\n",
            "loss =  0.5317  time = 2216.6977  n = 000150 |  0.8163     0.8250\n",
            "loss =  0.3840  time = 2231.4292  n = 000151 |  0.8192     0.8267\n",
            "loss =  0.6373  time = 2246.2199  n = 000152 |  0.8308     0.8344\n",
            "loss =  0.6804  time = 2261.1273  n = 000153 |  0.8326     0.8359\n",
            "loss =  0.5545  time = 2275.8907  n = 000154 |  0.8363     0.8425\n",
            "loss =  0.1896  time = 2290.8673  n = 000155 |  0.8327     0.8400\n",
            "loss =  0.4102  time = 2305.5045  n = 000156 |  0.8228     0.8285\n",
            "loss =  0.1554  time = 2320.4805  n = 000157 |  0.8278     0.8328\n",
            "loss =  0.3605  time = 2335.0423  n = 000158 |  0.8313     0.8360\n",
            "loss =  0.1547  time = 2349.5289  n = 000159 |  0.8300     0.8371\n",
            "loss =  0.2538  time = 2364.0318  n = 000160 |  0.8357     0.8407\n",
            "loss =  0.2423  time = 2378.7485  n = 000161 |  0.8368     0.8424\n",
            "loss =  0.1636  time = 2393.2420  n = 000162 |  0.8338     0.8425\n",
            "loss =  0.3281  time = 2407.8085  n = 000163 |  0.8312     0.8377\n",
            "loss =  0.1936  time = 2422.3294  n = 000164 |  0.8342     0.8419\n",
            "loss =  0.3656  time = 2437.0952  n = 000165 |  0.8336     0.8387\n",
            "loss =  0.6600  time = 2451.8515  n = 000166 |  0.8153     0.8186\n",
            "loss =  0.2726  time = 2466.5589  n = 000167 |  0.8246     0.8347\n",
            "loss =  0.4511  time = 2481.4881  n = 000168 |  0.8080     0.8143\n",
            "loss =  0.1738  time = 2496.6184  n = 000169 |  0.8155     0.8212\n",
            "loss =  0.3636  time = 2511.5528  n = 000170 |  0.8117     0.8182\n",
            "loss =  0.2412  time = 2526.4722  n = 000171 |  0.8310     0.8397\n",
            "loss =  0.2813  time = 2540.9928  n = 000172 |  0.8311     0.8396\n",
            "loss =  0.3240  time = 2555.8529  n = 000173 |  0.8148     0.8225\n",
            "loss =  0.5123  time = 2570.6219  n = 000174 |  0.8327     0.8399\n",
            "loss =  0.2533  time = 2585.0672  n = 000175 |  0.8340     0.8406\n",
            "loss =  0.1898  time = 2600.0194  n = 000176 |  0.8298     0.8359\n",
            "loss =  0.5859  time = 2615.0859  n = 000177 |  0.8195     0.8239\n",
            "loss =  0.5477  time = 2630.1023  n = 000178 |  0.8383     0.8468\n",
            "loss =  0.5618  time = 2644.8989  n = 000179 |  0.8352     0.8426\n",
            "loss =  0.3405  time = 2659.6083  n = 000180 |  0.8361     0.8415\n",
            "loss =  0.2513  time = 2674.7167  n = 000181 |  0.8381     0.8449\n",
            "loss =  0.3737  time = 2689.3843  n = 000182 |  0.8174     0.8215\n",
            "loss =  0.2320  time = 2704.1311  n = 000183 |  0.8330     0.8414\n",
            "loss =  0.3326  time = 2718.8128  n = 000184 |  0.8347     0.8417\n",
            "loss =  0.4883  time = 2733.6915  n = 000185 |  0.8374     0.8420\n",
            "loss =  0.2878  time = 2748.1771  n = 000186 |  0.8370     0.8405\n",
            "loss =  0.1766  time = 2763.1267  n = 000187 |  0.8245     0.8309\n",
            "loss =  0.1897  time = 2777.8519  n = 000188 |  0.8421     0.8454\n",
            "loss =  0.5662  time = 2793.0122  n = 000189 |  0.8239     0.8258\n",
            "loss =  0.3644  time = 2807.6389  n = 000190 |  0.8291     0.8335\n",
            "loss =  0.3567  time = 2822.2808  n = 000191 |  0.8194     0.8268\n",
            "loss =  0.2416  time = 2836.7525  n = 000192 |  0.8114     0.8113\n",
            "loss =  0.3554  time = 2851.6884  n = 000193 |  0.8266     0.8310\n",
            "loss =  0.2363  time = 2866.2722  n = 000194 |  0.8281     0.8317\n",
            "loss =  0.1603  time = 2881.0627  n = 000195 |  0.8296     0.8331\n",
            "loss =  0.2501  time = 2895.6170  n = 000196 |  0.8257     0.8291\n",
            "loss =  0.4878  time = 2910.9406  n = 000197 |  0.8353     0.8383\n",
            "loss =  0.3694  time = 2925.6950  n = 000198 |  0.8359     0.8402\n",
            "loss =  0.2804  time = 2940.2336  n = 000199 |  0.8276     0.8305\n",
            "loss =  0.3328  time = 2954.7819  n = 000200 |  0.8393     0.8448\n",
            "loss =  0.3648  time = 2969.4800  n = 000201 |  0.8118     0.8179\n",
            "loss =  0.1310  time = 2984.2146  n = 000202 |  0.8319     0.8373\n",
            "loss =  0.2255  time = 2998.8792  n = 000203 |  0.8397     0.8454\n",
            "loss =  0.3179  time = 3013.4580  n = 000204 |  0.8484     0.8493\n",
            "loss =  0.5587  time = 3028.5017  n = 000205 |  0.8330     0.8368\n",
            "loss =  0.3367  time = 3043.4845  n = 000206 |  0.8488     0.8509\n",
            "loss =  0.2762  time = 3058.1585  n = 000207 |  0.8452     0.8479\n",
            "loss =  0.3482  time = 3072.9088  n = 000208 |  0.8469     0.8510\n",
            "loss =  0.2618  time = 3087.5884  n = 000209 |  0.8360     0.8434\n",
            "loss =  0.3714  time = 3102.3489  n = 000210 |  0.7650     0.7690\n",
            "loss =  0.5518  time = 3117.0944  n = 000211 |  0.8321     0.8319\n",
            "loss =  0.6108  time = 3131.6806  n = 000212 |  0.8413     0.8417\n",
            "loss =  0.2838  time = 3146.5004  n = 000213 |  0.8562     0.8573\n",
            "loss =  0.2530  time = 3161.3158  n = 000214 |  0.8547     0.8563\n",
            "loss =  0.2025  time = 3175.9510  n = 000215 |  0.8512     0.8536\n",
            "loss =  0.2897  time = 3190.5856  n = 000216 |  0.8538     0.8549\n",
            "loss =  0.3612  time = 3205.1265  n = 000217 |  0.8499     0.8520\n",
            "loss =  0.4122  time = 3220.1626  n = 000218 |  0.8499     0.8541\n",
            "loss =  0.2658  time = 3234.9354  n = 000219 |  0.8525     0.8538\n",
            "loss =  0.1597  time = 3249.6714  n = 000220 |  0.8529     0.8533\n",
            "loss =  0.1334  time = 3264.5742  n = 000221 |  0.8375     0.8424\n",
            "loss =  0.1967  time = 3279.2996  n = 000222 |  0.8444     0.8505\n",
            "loss =  0.5049  time = 3294.0444  n = 000223 |  0.8317     0.8349\n",
            "loss =  0.1724  time = 3308.5304  n = 000224 |  0.8402     0.8470\n",
            "loss =  0.1372  time = 3323.1966  n = 000225 |  0.8292     0.8300\n",
            "loss =  0.2980  time = 3337.9825  n = 000226 |  0.8466     0.8513\n",
            "loss =  0.2726  time = 3352.7015  n = 000227 |  0.8531     0.8569\n",
            "loss =  0.0919  time = 3367.7834  n = 000228 |  0.8481     0.8515\n",
            "loss =  0.1960  time = 3382.3056  n = 000229 |  0.8545     0.8593\n",
            "loss =  0.1209  time = 3397.2558  n = 000230 |  0.8560     0.8608\n",
            "loss =  0.1716  time = 3412.0889  n = 000231 |  0.8542     0.8583\n",
            "loss =  0.3268  time = 3426.9671  n = 000232 |  0.8558     0.8621\n",
            "loss =  0.2724  time = 3441.7168  n = 000233 |  0.8527     0.8586\n",
            "loss =  0.3530  time = 3456.5425  n = 000234 |  0.8514     0.8550\n",
            "loss =  0.5316  time = 3471.3851  n = 000235 |  0.8521     0.8579\n",
            "loss =  0.4588  time = 3486.2089  n = 000236 |  0.8519     0.8549\n",
            "loss =  0.2400  time = 3500.9841  n = 000237 |  0.8583     0.8608\n",
            "loss =  0.1823  time = 3516.0372  n = 000238 |  0.8573     0.8591\n",
            "loss =  0.2389  time = 3531.3100  n = 000239 |  0.8619     0.8646\n",
            "loss =  0.1336  time = 3546.2382  n = 000240 |  0.8602     0.8615\n",
            "loss =  0.3696  time = 3561.0113  n = 000241 |  0.8595     0.8644\n",
            "loss =  0.1496  time = 3575.9324  n = 000242 |  0.8545     0.8579\n",
            "loss =  0.2990  time = 3590.7363  n = 000243 |  0.8554     0.8615\n",
            "loss =  0.2481  time = 3605.5950  n = 000244 |  0.8557     0.8640\n",
            "loss =  0.1790  time = 3620.0191  n = 000245 |  0.8577     0.8615\n",
            "loss =  0.1860  time = 3634.9621  n = 000246 |  0.8483     0.8530\n",
            "loss =  0.2240  time = 3649.5827  n = 000247 |  0.8506     0.8543\n",
            "loss =  0.2456  time = 3664.3971  n = 000248 |  0.8589     0.8606\n",
            "loss =  0.0989  time = 3678.8601  n = 000249 |  0.8532     0.8575\n",
            "loss =  0.1973  time = 3693.6535  n = 000250 |  0.8570     0.8623\n",
            "loss =  0.1551  time = 3708.3284  n = 000251 |  0.8556     0.8574\n",
            "loss =  0.1335  time = 3722.9899  n = 000252 |  0.8560     0.8582\n",
            "loss =  0.0645  time = 3737.7865  n = 000253 |  0.8581     0.8591\n",
            "loss =  0.1302  time = 3752.6083  n = 000254 |  0.8465     0.8500\n",
            "loss =  0.1676  time = 3767.4375  n = 000255 |  0.8517     0.8535\n",
            "loss =  0.3080  time = 3782.5370  n = 000256 |  0.8539     0.8558\n",
            "loss =  0.4486  time = 3797.5036  n = 000257 |  0.8556     0.8604\n",
            "loss =  0.5161  time = 3812.3911  n = 000258 |  0.8417     0.8444\n",
            "loss =  0.2085  time = 3827.0788  n = 000259 |  0.8458     0.8492\n",
            "loss =  0.4867  time = 3842.1728  n = 000260 |  0.8522     0.8582\n",
            "loss =  0.1414  time = 3857.1678  n = 000261 |  0.8363     0.8394\n",
            "loss =  0.3288  time = 3872.1053  n = 000262 |  0.8420     0.8481\n",
            "loss =  0.1925  time = 3886.8896  n = 000263 |  0.8286     0.8356\n",
            "loss =  0.2104  time = 3901.6329  n = 000264 |  0.8545     0.8580\n",
            "loss =  0.3186  time = 3916.2443  n = 000265 |  0.8596     0.8653\n",
            "loss =  0.2633  time = 3931.0152  n = 000266 |  0.8597     0.8656\n",
            "loss =  0.1487  time = 3945.4984  n = 000267 |  0.8587     0.8659\n",
            "loss =  0.0604  time = 3960.2104  n = 000268 |  0.8404     0.8474\n",
            "loss =  0.4161  time = 3974.7473  n = 000269 |  0.8521     0.8601\n",
            "loss =  0.2751  time = 3989.4160  n = 000270 |  0.8618     0.8688\n",
            "loss =  0.2003  time = 4004.3456  n = 000271 |  0.8598     0.8657\n",
            "loss =  0.2136  time = 4019.2344  n = 000272 |  0.8318     0.8380\n",
            "loss =  0.1984  time = 4033.7830  n = 000273 |  0.8636     0.8684\n",
            "loss =  0.4357  time = 4048.3171  n = 000274 |  0.8445     0.8508\n",
            "loss =  0.3591  time = 4063.1556  n = 000275 |  0.8525     0.8586\n",
            "loss =  0.1737  time = 4077.8176  n = 000276 |  0.8602     0.8651\n",
            "loss =  0.2533  time = 4092.6230  n = 000277 |  0.8518     0.8562\n",
            "loss =  0.5490  time = 4107.2645  n = 000278 |  0.8343     0.8385\n",
            "loss =  0.1853  time = 4122.0467  n = 000279 |  0.8555     0.8588\n",
            "loss =  0.3631  time = 4136.6744  n = 000280 |  0.8542     0.8548\n",
            "loss =  0.2204  time = 4151.7385  n = 000281 |  0.8531     0.8520\n",
            "loss =  0.2146  time = 4166.5195  n = 000282 |  0.8493     0.8511\n",
            "loss =  0.0974  time = 4181.3044  n = 000283 |  0.8588     0.8624\n",
            "loss =  0.2322  time = 4195.7096  n = 000284 |  0.8620     0.8627\n",
            "loss =  0.1338  time = 4210.7526  n = 000285 |  0.8624     0.8650\n",
            "loss =  0.3277  time = 4225.4898  n = 000286 |  0.8380     0.8379\n",
            "loss =  0.1705  time = 4240.4235  n = 000287 |  0.8581     0.8611\n",
            "loss =  0.1647  time = 4255.4585  n = 000288 |  0.8668     0.8697\n",
            "loss =  0.2348  time = 4270.4557  n = 000289 |  0.8686     0.8712\n",
            "loss =  0.1192  time = 4285.1197  n = 000290 |  0.8640     0.8673\n",
            "loss =  0.2326  time = 4300.1595  n = 000291 |  0.8674     0.8694\n",
            "loss =  0.2358  time = 4315.2125  n = 000292 |  0.8675     0.8710\n",
            "loss =  0.2586  time = 4330.3967  n = 000293 |  0.8633     0.8681\n",
            "loss =  0.1736  time = 4345.4559  n = 000294 |  0.8616     0.8632\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}