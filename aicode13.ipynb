{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aicode13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYIx5O+FuXEP5Ik4Jo+YAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eventia/mnistcode/blob/master/aicode13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycl3JfN_pAD6",
        "colab_type": "text"
      },
      "source": [
        "# 고등학교 수학과 파이썬으로 배우는 인공지능 소스코드  **Ch 13**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4vEpafuo4Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 13.6. 오차역전파를 사용한 MNIST 학습\n",
        "\n",
        "# 13.6.1. [STEP1] 미분과 역전파 선택\n",
        "\n",
        "# 오차역전파와 미분함수 중 선택\n",
        "# process = (미분사용 : 1 , 역전파사용 : 2)\n",
        "\n",
        "process = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJGE-zp3rW4j",
        "colab_type": "code",
        "outputId": "bd787ce3-ec4d-459d-82e8-eebd4f09cc9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# 13.6.2. [STEP2] MNIST 데이터 가져오기\n",
        "\n",
        "import numpy as np \n",
        "import time\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "t_trainlbl, t_testlbl = t_train, t_test\n",
        "\n",
        "# 28X28 을 784 로 수정\n",
        "x_train = x_train.reshape(60000,784)    # 주석 (1)\n",
        "x_test = x_test.reshape(10000,784)    \n",
        "\n",
        "# one-hot label \n",
        "T0 = np.zeros((t_train.size, 10))    #(60000,10) = 000\n",
        "T1 = np.zeros((t_test.size, 10))    #(10000,10) = 000\n",
        "\n",
        "for idx in range(t_train.size): T0[idx][t_train[idx]] = 1    #(3))\n",
        "for idx in range(t_test.size): T1[idx][t_test[idx]] = 1\n",
        "\n",
        "t_train, t_test = T0, T1\n",
        "\n",
        "# normalize 0.0 ~ 1.0\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "print('MNIST DataSets 준비 완료')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "MNIST DataSets 준비 완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50hxeoihrRe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 13.6.3. [STEP3] 함수 정의 : 수치미분, 소프트맥스, CEE\n",
        "\n",
        "# 미분함수 \n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4    # 0.0001\n",
        "    nd_coef = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        index = it.multi_index\n",
        "        tmp = float(x[index])\n",
        "        x[index] = tmp + h\n",
        "        fxh2 = f()    # f(x+h)\n",
        "        x[index] = tmp - h \n",
        "        fxh1 = f()    # f(x-h)\n",
        "        nd_coef[index] = (fxh2 - fxh1) / (2*h)\n",
        "        x[index] = tmp \n",
        "        it.iternext()\n",
        "    return nd_coef\n",
        "\n",
        "# 소프트맥스\n",
        "def softmax(x):\n",
        "    if x.ndim == 1:  # 기본 1개 처리과정 , 벡터입력\n",
        "        x = x - np.max(x) \n",
        "        return np.exp(x) / np.sum(np.exp(x))\n",
        "    if x.ndim == 2:  # 배치용 n 개 처리, 행렬입력\n",
        "        x = x.T - np.max(x.T, axis=0)\n",
        "        return (np.exp(x) / np.sum(np.exp(x), axis=0)).T\n",
        "\n",
        "# 크로스엔트로피오차\n",
        "def cee(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)  # 크기가 1xN 인 2차원 행렬로 재구성\n",
        "        y = y.reshape(1, y.size)\n",
        "    result = -np.sum(t * np.log(y + 1e-7)) / y.shape[0]\n",
        "    return result \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPM_52X0rN5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 13.6.4. [STEP4] 클래스 정의 : ReLU, Affine, SoftmaxWithLoss, \n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        result = x.copy()\n",
        "        result[self.mask] = 0\n",
        "        return result\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W    # W0, W1\n",
        "        self.b = b    # b0, b1\n",
        "        self.x = None\n",
        "        self.dW = None    # W0, W1 의 기울기\n",
        "        self.db = None    # b0, b1 의 기울기\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        result = np.dot(self.x, self.W) + self.b\n",
        "        return result\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        return dx\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.y = None    # 출력(계산결과)\n",
        "        self.t = None    # 정답(MNIST레이블)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        result = cee(self.y, self.t)\n",
        "        return result\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRX6fx0oqo1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 13.6.5. [STEP5] 클래스 정의 : SimpleNetwork\n",
        "\n",
        "class SimpleNetwork:\n",
        "    def __init__(self, inputx, hidden, outy, weight):\n",
        "        # 가중치 초기화\n",
        "        self.netMat = {}\n",
        "        self.netMat['W0'] = weight * np.random.randn(inputx, hidden)\n",
        "        self.netMat['b0'] = np.zeros(hidden)\n",
        "        self.netMat['W1'] = weight * np.random.randn(hidden, outy) \n",
        "        self.netMat['b1'] = np.zeros(outy)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.netLayers = {}\n",
        "        self.netLayers['Affine1'] = Affine(self.netMat['W0'], \n",
        "                                           self.netMat['b0'])\n",
        "        self.netLayers['Relu1'] = Relu()\n",
        "        self.netLayers['Affine2'] = Affine(self.netMat['W1'], \n",
        "                                           self.netMat['b1'])\n",
        "        self.netLayers['Softmax'] = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = self.netLayers['Affine1'].forward(x)\n",
        "        x = self.netLayers['Relu1'].forward(x)\n",
        "        x = self.netLayers['Affine2'].forward(x)\n",
        "        return x\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.netLayers['Softmax'].forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    def numerical_gradient(self, x, t):\n",
        "        lossfunc = lambda : self.loss(x, t)\n",
        "        grads = {}\n",
        "        grads['W0'] = numerical_diff(lossfunc, self.netMat['W0'])\n",
        "        grads['b0'] = numerical_diff(lossfunc, self.netMat['b0'])\n",
        "        grads['W1'] = numerical_diff(lossfunc, self.netMat['W1'])\n",
        "        grads['b1'] = numerical_diff(lossfunc, self.netMat['b1'])\n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.netLayers['Softmax'].backward(dout)\n",
        "        dout = self.netLayers['Affine2'].backward(dout)\n",
        "        dout = self.netLayers['Relu1'].backward(dout)\n",
        "        dout = self.netLayers['Affine1'].backward(dout)\n",
        "\n",
        "        # 기울기(dW, db) 저장\n",
        "        grads = {}\n",
        "        grads['W0'] = self.netLayers['Affine1'].dW \n",
        "        grads['b0'] = self.netLayers['Affine1'].db\n",
        "        grads['W1'] = self.netLayers['Affine2'].dW \n",
        "        grads['b1'] = self.netLayers['Affine2'].db\n",
        "        return grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Ahz-BZq1TC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 13.6.6. [SETP6] 학습을 위한 설정치 입력\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "lr = 0.1\n",
        "iter = 0\n",
        "\n",
        "# 미분을 사용할 경우 :: 배치 20, 1000회 반복 \n",
        "# (20개 묶음 데이터로 1000번 학습진행)\n",
        "if process == 1:\n",
        "    iters_num = 1000\n",
        "    batch_size = 20\n",
        "    iter_per_epoch = 1\n",
        "\n",
        "# 역전파사용 : 배치 100, 60000회 반복\n",
        "# 100개 묶음 데이터로 60000 회 학습진행\n",
        "else :\n",
        "    iters_num = 60000\n",
        "    batch_size = 100\n",
        "    iter_per_epoch = int(train_size / batch_size)    # 600\n",
        "\n",
        "# MNIST 입력(784), 은닉층(노드 50개), 출력층(노드 10개)\n",
        "network = SimpleNetwork(inputx=784, hidden=50, outy=10, weight = 0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkrNeDO4q_1F",
        "colab_type": "code",
        "outputId": "33f7963c-66af-4a01-f613-1fa1b436b435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 13.6.7. [STEP7] 학습과 검증 \n",
        "\n",
        "# 시간측정 시작\n",
        "t1 = time.time()\n",
        "print('loss = _______  time = ________  n = ______ | [TrainAcc] [TestAcc]')\n",
        "\n",
        "for i in range(iters_num):   \n",
        "    batch_mask = np.random.choice(train_size, batch_size)    # 60000 개중 100 개\n",
        "    x_batch = x_train[batch_mask]    \n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "\n",
        "    if process==1:\n",
        "        grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    else:\n",
        "        grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "    \n",
        "    # 위에서 만들어진 기울기로 W 와 b 갱신\n",
        "    for key in ('W0', 'b0', 'W1', 'b1'):\n",
        "        network.netMat[key] -=  lr * grad[key] \n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    # train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        iter = iter + 1\n",
        "        print('loss = {:7.4f}  '.format(loss), end='')\n",
        "        print('time = {:8.4f}  '.format(time.time()-t1), end='')    \n",
        "        print('n = {:06d} |{:8.4f}{:11.4f}'.format(iter, train_acc, test_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss = _______  time = ________  n = ______ | [TrainAcc] [TestAcc]\n",
            "loss =  2.7540  time =   0.4952  n = 000001 |  0.1506     0.1503\n",
            "loss =  0.1574  time =   2.8475  n = 000002 |  0.9027     0.9095\n",
            "loss =  0.1694  time =   5.2220  n = 000003 |  0.9269     0.9295\n",
            "loss =  0.1173  time =   8.3149  n = 000004 |  0.9372     0.9402\n",
            "loss =  0.1112  time =  10.6638  n = 000005 |  0.9447     0.9450\n",
            "loss =  0.1666  time =  12.9969  n = 000006 |  0.9510     0.9489\n",
            "loss =  0.2669  time =  15.3429  n = 000007 |  0.9545     0.9485\n",
            "loss =  0.1235  time =  17.6230  n = 000008 |  0.9592     0.9526\n",
            "loss =  0.1266  time =  19.9920  n = 000009 |  0.9601     0.9543\n",
            "loss =  0.0255  time =  22.3649  n = 000010 |  0.9631     0.9562\n",
            "loss =  0.1276  time =  24.7273  n = 000011 |  0.9661     0.9602\n",
            "loss =  0.1735  time =  27.0809  n = 000012 |  0.9679     0.9610\n",
            "loss =  0.0872  time =  29.4375  n = 000013 |  0.9695     0.9627\n",
            "loss =  0.1155  time =  31.8123  n = 000014 |  0.9714     0.9638\n",
            "loss =  0.0339  time =  34.1499  n = 000015 |  0.9725     0.9657\n",
            "loss =  0.0649  time =  36.5588  n = 000016 |  0.9739     0.9666\n",
            "loss =  0.0237  time =  38.8788  n = 000017 |  0.9751     0.9673\n",
            "loss =  0.0685  time =  41.2243  n = 000018 |  0.9750     0.9647\n",
            "loss =  0.0467  time =  43.4922  n = 000019 |  0.9770     0.9692\n",
            "loss =  0.0237  time =  45.8956  n = 000020 |  0.9775     0.9694\n",
            "loss =  0.0875  time =  48.2263  n = 000021 |  0.9779     0.9687\n",
            "loss =  0.1110  time =  50.5798  n = 000022 |  0.9785     0.9674\n",
            "loss =  0.0752  time =  52.9463  n = 000023 |  0.9796     0.9681\n",
            "loss =  0.0398  time =  55.2945  n = 000024 |  0.9805     0.9715\n",
            "loss =  0.0695  time =  57.6958  n = 000025 |  0.9815     0.9690\n",
            "loss =  0.0205  time =  60.0545  n = 000026 |  0.9826     0.9713\n",
            "loss =  0.0068  time =  62.4362  n = 000027 |  0.9830     0.9705\n",
            "loss =  0.0356  time =  64.8354  n = 000028 |  0.9839     0.9721\n",
            "loss =  0.0744  time =  67.3224  n = 000029 |  0.9841     0.9714\n",
            "loss =  0.0147  time =  69.7391  n = 000030 |  0.9850     0.9729\n",
            "loss =  0.0814  time =  72.1238  n = 000031 |  0.9851     0.9724\n",
            "loss =  0.1465  time =  74.4584  n = 000032 |  0.9860     0.9714\n",
            "loss =  0.0259  time =  76.8896  n = 000033 |  0.9862     0.9728\n",
            "loss =  0.0814  time =  79.2392  n = 000034 |  0.9866     0.9735\n",
            "loss =  0.0212  time =  81.5918  n = 000035 |  0.9872     0.9734\n",
            "loss =  0.0144  time =  83.8932  n = 000036 |  0.9875     0.9732\n",
            "loss =  0.0185  time =  86.2345  n = 000037 |  0.9873     0.9719\n",
            "loss =  0.0941  time =  88.6157  n = 000038 |  0.9882     0.9729\n",
            "loss =  0.0383  time =  90.9731  n = 000039 |  0.9886     0.9736\n",
            "loss =  0.0265  time =  93.3253  n = 000040 |  0.9872     0.9723\n",
            "loss =  0.0227  time =  95.6808  n = 000041 |  0.9899     0.9748\n",
            "loss =  0.1168  time =  98.0249  n = 000042 |  0.9898     0.9744\n",
            "loss =  0.0246  time = 100.3597  n = 000043 |  0.9905     0.9744\n",
            "loss =  0.0133  time = 102.6608  n = 000044 |  0.9903     0.9733\n",
            "loss =  0.0200  time = 104.9995  n = 000045 |  0.9906     0.9739\n",
            "loss =  0.0342  time = 107.3122  n = 000046 |  0.9917     0.9744\n",
            "loss =  0.0123  time = 109.7091  n = 000047 |  0.9919     0.9739\n",
            "loss =  0.0188  time = 112.0701  n = 000048 |  0.9919     0.9745\n",
            "loss =  0.0132  time = 114.4150  n = 000049 |  0.9919     0.9738\n",
            "loss =  0.0058  time = 116.7489  n = 000050 |  0.9915     0.9727\n",
            "loss =  0.0168  time = 119.1381  n = 000051 |  0.9920     0.9740\n",
            "loss =  0.0187  time = 121.4632  n = 000052 |  0.9927     0.9735\n",
            "loss =  0.0136  time = 123.7982  n = 000053 |  0.9931     0.9725\n",
            "loss =  0.0051  time = 126.1364  n = 000054 |  0.9934     0.9734\n",
            "loss =  0.0113  time = 128.5366  n = 000055 |  0.9938     0.9727\n",
            "loss =  0.0193  time = 130.8624  n = 000056 |  0.9938     0.9741\n",
            "loss =  0.0062  time = 133.1974  n = 000057 |  0.9940     0.9728\n",
            "loss =  0.0093  time = 135.4851  n = 000058 |  0.9940     0.9725\n",
            "loss =  0.0167  time = 137.8415  n = 000059 |  0.9942     0.9727\n",
            "loss =  0.0126  time = 140.2267  n = 000060 |  0.9943     0.9742\n",
            "loss =  0.0098  time = 142.5865  n = 000061 |  0.9957     0.9740\n",
            "loss =  0.0084  time = 144.8771  n = 000062 |  0.9947     0.9735\n",
            "loss =  0.0235  time = 147.2255  n = 000063 |  0.9953     0.9732\n",
            "loss =  0.0243  time = 149.6068  n = 000064 |  0.9950     0.9730\n",
            "loss =  0.0177  time = 151.9739  n = 000065 |  0.9955     0.9739\n",
            "loss =  0.0211  time = 154.3523  n = 000066 |  0.9958     0.9737\n",
            "loss =  0.0064  time = 156.7049  n = 000067 |  0.9962     0.9729\n",
            "loss =  0.0097  time = 159.0639  n = 000068 |  0.9963     0.9743\n",
            "loss =  0.0074  time = 161.5835  n = 000069 |  0.9960     0.9744\n",
            "loss =  0.0065  time = 163.9920  n = 000070 |  0.9961     0.9742\n",
            "loss =  0.0066  time = 166.3566  n = 000071 |  0.9964     0.9740\n",
            "loss =  0.0049  time = 168.6974  n = 000072 |  0.9968     0.9739\n",
            "loss =  0.0031  time = 171.1242  n = 000073 |  0.9969     0.9740\n",
            "loss =  0.0125  time = 173.4682  n = 000074 |  0.9969     0.9736\n",
            "loss =  0.0111  time = 175.8326  n = 000075 |  0.9970     0.9733\n",
            "loss =  0.0089  time = 178.1397  n = 000076 |  0.9970     0.9741\n",
            "loss =  0.0042  time = 180.3980  n = 000077 |  0.9974     0.9734\n",
            "loss =  0.0058  time = 182.7621  n = 000078 |  0.9974     0.9728\n",
            "loss =  0.0058  time = 185.0109  n = 000079 |  0.9976     0.9741\n",
            "loss =  0.0048  time = 187.3553  n = 000080 |  0.9977     0.9727\n",
            "loss =  0.0070  time = 189.6838  n = 000081 |  0.9978     0.9737\n",
            "loss =  0.0044  time = 192.0616  n = 000082 |  0.9979     0.9735\n",
            "loss =  0.0130  time = 194.3610  n = 000083 |  0.9980     0.9734\n",
            "loss =  0.0023  time = 196.7209  n = 000084 |  0.9979     0.9741\n",
            "loss =  0.0326  time = 199.0452  n = 000085 |  0.9980     0.9731\n",
            "loss =  0.0076  time = 201.4038  n = 000086 |  0.9983     0.9730\n",
            "loss =  0.0079  time = 203.7310  n = 000087 |  0.9981     0.9729\n",
            "loss =  0.0074  time = 206.0531  n = 000088 |  0.9982     0.9735\n",
            "loss =  0.0049  time = 208.3514  n = 000089 |  0.9985     0.9745\n",
            "loss =  0.0040  time = 210.6980  n = 000090 |  0.9988     0.9735\n",
            "loss =  0.0069  time = 213.1394  n = 000091 |  0.9985     0.9734\n",
            "loss =  0.0662  time = 215.4891  n = 000092 |  0.9984     0.9721\n",
            "loss =  0.0041  time = 217.7727  n = 000093 |  0.9985     0.9732\n",
            "loss =  0.0076  time = 220.1232  n = 000094 |  0.9988     0.9743\n",
            "loss =  0.0030  time = 222.4747  n = 000095 |  0.9987     0.9729\n",
            "loss =  0.0044  time = 224.7854  n = 000096 |  0.9989     0.9748\n",
            "loss =  0.0033  time = 227.0741  n = 000097 |  0.9989     0.9741\n",
            "loss =  0.0047  time = 229.3955  n = 000098 |  0.9989     0.9723\n",
            "loss =  0.0018  time = 231.7223  n = 000099 |  0.9989     0.9728\n",
            "loss =  0.0049  time = 234.1157  n = 000100 |  0.9992     0.9737\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}